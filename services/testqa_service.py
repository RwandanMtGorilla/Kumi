import yaml
import re
from llm.factory import LLMFactory
import json
import pandas as pd
import requests
import os
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from threading import Lock
from config.settings import settings


class ModelEvaluator:
    def __init__(self,
                 yaml_folder=None,
                 output_folder=None):

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç»å¯¹è·¯å¾„ï¼Œå¦‚æœä¼ å…¥å‚æ•°åˆ™ä½¿ç”¨å‚æ•°
        self.yaml_folder = yaml_folder or settings.YAML_RULES_PATH
        self.output_folder = output_folder or settings.EVALUATION_RESULTS_PATH

        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
        Path(self.output_folder).mkdir(parents=True, exist_ok=True)

        # æ‰“å°å½“å‰ä½¿ç”¨çš„è·¯å¾„
        print(f"ğŸ“ YAMLè§„åˆ™æ–‡ä»¶å¤¹è·¯å¾„: {self.yaml_folder}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„: {self.output_folder}")

        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        try:
            self.llm_client = LLMFactory.create_client()
        except Exception as e:
            print(f"âŒ åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.llm_client = None

        self.lock = Lock()

    def _normalize_text(self, text):
        """å¤„ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦"""
        if not isinstance(text, str):
            text = str(text)

        # å¤„ç†å¸¸è§çš„æ¢è¡Œç¬¦ç¼–ç 
        text = text.replace('\\n', '\n')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \n
        text = text.replace('\\r', '\r')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \r
        text = text.replace('\\t', '\t')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \t

        # å¤„ç†å…¶ä»–å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦
        text = text.replace('\\"', '"')  # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
        text = text.replace("\\'", "'")  # å¤„ç†è½¬ä¹‰çš„å•å¼•å·

        return text

    def _load_yaml_rule(self, yaml_filename):
        """åŠ è½½YAMLè¯„æµ‹è§„åˆ™"""
        yaml_path = os.path.join(self.yaml_folder, yaml_filename)
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"YAMLæ–‡ä»¶ {yaml_path} ä¸å­˜åœ¨")

        try:
            with open(yaml_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"YAMLæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")

    def _fill_prompt_template(self, prompt_template, variables):
        """å¡«å……æç¤ºè¯æ¨¡æ¿"""
        filled_prompt = prompt_template

        for key, value in variables.items():
            placeholder = f"{{{{{key}}}}}"
            if placeholder in filled_prompt:
                filled_prompt = filled_prompt.replace(placeholder, str(value))

        return filled_prompt

    def _extract_json_from_response(self, response_text):
        """ä»æ¨¡å‹å›ç­”ä¸­æå–JSON"""
        if not response_text:
            return {"score": 0.0, "reason": "æ¨¡å‹æ— å›åº”"}

        # å…ˆè§„èŒƒåŒ–æ–‡æœ¬
        response_text = self._normalize_text(response_text)

        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            result = json.loads(response_text.strip())
            # å¯¹ç»“æœä¸­çš„å­—ç¬¦ä¸²å­—æ®µè¿›è¡Œè§„èŒƒåŒ–å¤„ç†
            if isinstance(result, dict):
                for key, value in result.items():
                    if isinstance(value, str):
                        result[key] = self._normalize_text(value)
            return result
        except json.JSONDecodeError:
            pass

        # å°è¯•æå–JSONä»£ç å—
        json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        matches = re.findall(json_pattern, response_text, re.DOTALL | re.IGNORECASE)
        if matches:
            try:
                result = json.loads(matches[0])
                if isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, str):
                            result[key] = self._normalize_text(value)
                return result
            except json.JSONDecodeError:
                pass

        # å°è¯•æå–èŠ±æ‹¬å·å†…çš„å†…å®¹
        brace_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(brace_pattern, response_text, re.DOTALL)
        for match in matches:
            try:
                result = json.loads(match)
                if isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, str):
                            result[key] = self._normalize_text(value)
                return result
            except json.JSONDecodeError:
                continue

        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå°è¯•åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„JSONç»“æ„
        score_pattern = r'"?score"?\s*:\s*([0-9.]+)'
        reason_pattern = r'"?reason"?\s*:\s*"([^"]*)"'

        score_match = re.search(score_pattern, response_text)
        reason_match = re.search(reason_pattern, response_text)

        if score_match:
            result = {"score": float(score_match.group(1))}
            if reason_match:
                result["reason"] = self._normalize_text(reason_match.group(1))
            else:
                result["reason"] = "æœªæ‰¾åˆ°å…·ä½“åŸå› "
            return result

        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        return {
            "score": 0.0,
            "reason": self._normalize_text(f"æ— æ³•è§£æè¯„æµ‹ç»“æœ: {response_text[:100]}...")
        }

    def list_yaml_rules(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„YAMLè§„åˆ™æ–‡ä»¶"""
        if not os.path.exists(self.yaml_folder):
            Path(self.yaml_folder).mkdir(parents=True, exist_ok=True)
            return []

        yaml_files = [f for f in os.listdir(self.yaml_folder) if f.endswith('.yaml') or f.endswith('.yml')]
        return yaml_files

    def evaluate_results(self, result_file_path, yaml_filename, progress_callback=None, max_workers=3):
        """
        è¯„æµ‹æ¨¡å‹ç»“æœ

        Args:
            result_file_path: ä¸Šä¸€æ­¥ç”Ÿæˆçš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
            yaml_filename: YAMLè¯„æµ‹è§„åˆ™æ–‡ä»¶å
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total, message)å‚æ•°

        Returns:
            str: è¯„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
        """
        if self.llm_client is None:
            raise RuntimeError("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œè¯„æµ‹")

        # åŠ è½½YAMLè§„åˆ™
        rule_config = self._load_yaml_rule(yaml_filename)

        # éªŒè¯YAMLè§„åˆ™æ ¼å¼
        if 'evaluation_rule' not in rule_config:
            raise ValueError("YAMLæ–‡ä»¶ä¸­ç¼ºå°‘ 'evaluation_rule' é…ç½®")

        if 'prompt' not in rule_config['evaluation_rule']:
            raise ValueError("YAMLæ–‡ä»¶ä¸­ç¼ºå°‘ 'prompt' é…ç½®")

        # è¯»å–ç»“æœæ–‡ä»¶
        if not os.path.exists(result_file_path):
            raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ {result_file_path} ä¸å­˜åœ¨")

        try:
            if result_file_path.endswith('.xlsx'):
                df = pd.read_excel(result_file_path)
            elif result_file_path.endswith('.csv'):
                # å°è¯•ä¸åŒçš„ç¼–ç 
                for encoding in ['utf-8', 'gbk', 'gb2312']:
                    try:
                        df = pd.read_csv(result_file_path, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise ValueError(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç¼–ç æ ¼å¼")
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .xlsx æˆ– .csv æ–‡ä»¶")
        except Exception as e:
            raise ValueError(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")

        total_records = len(df)
        start_time = time.time()
        completed_count = 0
        evaluation_results = [None] * total_records  # é¢„åˆ†é…ç»“æœåˆ—è¡¨

        print(f"ğŸ” å¼€å§‹è¯„æµ‹ {total_records} æ¡è®°å½• (å¹¶å‘æ•°: {max_workers})")
        print(f"ğŸ“‹ ä½¿ç”¨è§„åˆ™: {rule_config['evaluation_rule']['name']}")
        print(f"ğŸ“ YAMLæ–‡ä»¶: {os.path.join(self.yaml_folder, yaml_filename)}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file_path}")
        print(f"ğŸ“¤ è¾“å‡ºè·¯å¾„: {self.output_folder}")
        print("-" * 60)

        if progress_callback:
            progress_callback(0, total_records, "å¼€å§‹è¯„æµ‹ä»»åŠ¡", 0)

            # è·å–è¯„æµ‹æç¤ºè¯æ¨¡æ¿
        prompt_template = rule_config['evaluation_rule']['prompt']['role']['description']

        def evaluate_single_record(args):
            """è¯„æµ‹å•æ¡è®°å½•çš„å‡½æ•°"""
            index, row = args

            try:
                # å‡†å¤‡å˜é‡å­—å…¸
                variables = {
                    'input': str(row.get('input', '')),
                    'generated_output': str(row.get('model_output', '')),
                    'reference_output': str(row.get('reference_output', '')),
                    'group': str(row.get('group', '')),
                }

                # æ·»åŠ æ‰€æœ‰åŸå§‹åˆ—ä½œä¸ºå¯èƒ½çš„å˜é‡
                for col in df.columns:
                    if col not in variables:
                        variables[col] = str(row.get(col, ''))

                # å¡«å……æç¤ºè¯
                filled_prompt = self._fill_prompt_template(prompt_template, variables)

                # è°ƒç”¨LLMè¿›è¡Œè¯„æµ‹
                evaluation_response = self.llm_client.simple_chat(
                    filled_prompt,
                    max_tokens=500,
                    temperature=rule_config.get('model_config', {}).get('temperature', 0.5)
                )

                # è§£æè¯„æµ‹ç»“æœ
                evaluation_json = self._extract_json_from_response(evaluation_response)

                # æ„å»ºæœ€ç»ˆç»“æœ
                result_row = row.to_dict()
                result_row['evaluation_score'] = evaluation_json.get('score', 0.0)
                result_row['evaluation_reason'] = evaluation_json.get('reason', 'æ— åŸå› ')
                result_row['evaluation_rule'] = rule_config['evaluation_rule']['name']
                result_row['evaluation_timestamp'] = datetime.now().isoformat()

                return index, result_row, None

            except Exception as e:
                error_msg = f"è¯„æµ‹ç¬¬ {index + 1} æ¡è®°å½•å¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")

                # æ„å»ºé”™è¯¯ç»“æœ
                result_row = row.to_dict()
                result_row['evaluation_score'] = 0.0
                result_row['evaluation_reason'] = f"è¯„æµ‹è¿‡ç¨‹å‡ºé”™: {str(e)}"
                result_row['evaluation_rule'] = rule_config['evaluation_rule']['name']
                result_row['evaluation_timestamp'] = datetime.now().isoformat()

                return index, result_row, str(e)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix='evaluation_thread') as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(evaluate_single_record, (index, row)): index
                for index, row in df.iterrows()
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_index):
                try:
                    index, result_row, error = future.result()
                    evaluation_results[index] = result_row

                    with self.lock:
                        completed_count += 1

                        # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
                        elapsed_time = time.time() - start_time
                        if completed_count > 0:
                            avg_time_per_record = elapsed_time / completed_count
                            remaining_records = total_records - completed_count
                            estimated_remaining_time = avg_time_per_record * remaining_records
                        else:
                            estimated_remaining_time = 0

                        print(
                            f"âœ… è¯„æµ‹å®Œæˆç¬¬ {completed_count}/{total_records} æ¡ - åˆ†æ•°: {result_row.get('evaluation_score', 'N/A')}")

                        # æ›´æ–°è¿›åº¦å›è°ƒ
                        if progress_callback:
                            progress_callback(
                                completed_count,
                                total_records,
                                f"å·²å®Œæˆç¬¬ {completed_count}/{total_records} æ¡è®°å½•è¯„æµ‹",
                                estimated_remaining_time
                            )

                except Exception as e:
                    print(f"âŒ å¤„ç†è¯„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")

        # ä¿å­˜æœ€ç»ˆè¯„æµ‹ç»“æœ
        final_results = [r for r in evaluation_results if r is not None]
        final_df = pd.DataFrame(final_results)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.basename(result_file_path).split('.')[0]
        rule_name = rule_config['evaluation_rule']['name'].replace(' ', '_')
        safe_rule_name = "".join(c for c in rule_name if c.isalnum() or c in (' ', '-', '_')).rstrip()

        output_filename = f"evaluation_{base_name}_{safe_rule_name}_{timestamp}.xlsx"
        output_path = os.path.join(self.output_folder, output_filename)

        final_df.to_excel(output_path, index=False, engine='openpyxl')

        # æœ€ç»ˆå®Œæˆå›è°ƒ
        if progress_callback:
            progress_callback(total_records, total_records, "è¯„æµ‹å®Œæˆï¼Œæ­£åœ¨ä¿å­˜ç»“æœæ–‡ä»¶", 0)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        avg_score = final_df['evaluation_score'].mean()
        max_score = final_df['evaluation_score'].max()
        min_score = final_df['evaluation_score'].min()

        print(f"\nğŸ“ˆ è¯„æµ‹å®Œæˆç»Ÿè®¡:")
        print(f"   å¹³å‡åˆ†æ•°: {avg_score:.2f}")
        print(f"   æœ€é«˜åˆ†æ•°: {max_score:.2f}")
        print(f"   æœ€ä½åˆ†æ•°: {min_score:.2f}")
        print(f"   æ€»è®°å½•æ•°: {len(final_df)}")
        print(f"   ç»“æœä¿å­˜åˆ°: {output_path}")

        return output_path


class DifyWorkflowCaller:
    def __init__(self,
                 config_file=None,
                 table_folder=None,
                 output_folder=None):

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç»å¯¹è·¯å¾„ï¼Œå¦‚æœä¼ å…¥å‚æ•°åˆ™ä½¿ç”¨å‚æ•°
        self.config_file = config_file or settings.WORKFLOWS_CONFIG_PATH
        self.table_folder = table_folder or settings.TABLE_FOLDER_PATH
        self.output_folder = output_folder or settings.TEMP_RESULTS_PATH

        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
        Path(self.output_folder).mkdir(parents=True, exist_ok=True)

        # æ‰“å°å½“å‰ä½¿ç”¨çš„è·¯å¾„
        print(f"ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {self.config_file}")
        print(f"ğŸ“ è¡¨æ ¼æ–‡ä»¶å¤¹è·¯å¾„: {self.table_folder}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„: {self.output_folder}")

        # åŠ è½½å·¥ä½œæµé…ç½®
        self.workflows = self._load_workflows()
        self.lock = Lock()

    def _call_dify_api_with_retry(self, workflow_config, query, inputs=None, max_retries=3):
        """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
        for attempt in range(max_retries):
            try:
                return self._call_dify_api(workflow_config, query, inputs)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’

    def _normalize_text(self, text):
        """å¤„ç†æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦"""
        if not isinstance(text, str):
            text = str(text)

        # å¤„ç†å¸¸è§çš„æ¢è¡Œç¬¦ç¼–ç 
        text = text.replace('\\n', '\n')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \n
        text = text.replace('\\r', '\r')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \r
        text = text.replace('\\t', '\t')  # å¤„ç†å­—é¢æ„ä¹‰çš„ \t

        # å¤„ç†å…¶ä»–å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦
        text = text.replace('\\"', '"')  # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
        text = text.replace("\\'", "'")  # å¤„ç†è½¬ä¹‰çš„å•å¼•å·

        return text

    def _load_workflows(self):
        """åŠ è½½å·¥ä½œæµé…ç½®"""
        try:
            if not os.path.exists(self.config_file):
                print(f"âŒ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
                # å°è¯•åˆ›å»ºé…ç½®æ–‡ä»¶çš„ç›®å½•
                Path(self.config_file).parent.mkdir(parents=True, exist_ok=True)
                return {}

            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return {wf['name']: wf for wf in config['workflows']}
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def _call_dify_api(self, workflow_config, query, inputs=None):
        """è°ƒç”¨Dify API"""
        headers = {
            "Authorization": f"Bearer {workflow_config['api_key']}",
            "Content-Type": "application/json"
        }

        data = {
            "inputs": inputs or {},  # æ”¯æŒé¢å¤– inputs
            "query": query,
            "response_mode": "blocking",
            "conversation_id": "",
            "user": "abc-123"
        }

        try:
            response = requests.post(workflow_config['url'], headers=headers, json=data, timeout=60)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

    def process_table(self, table_filename, workflow_name, progress_callback=None, max_workers=5):
        """
        å¤„ç†è¡¨æ ¼æ–‡ä»¶ï¼Œè°ƒç”¨å·¥ä½œæµAPIï¼ˆæ”¯æŒå¹¶å‘ï¼‰

        Args:
            table_filename: è¡¨æ ¼æ–‡ä»¶å
            workflow_name: å·¥ä½œæµåç§°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total, message, estimated_time)å‚æ•°
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # æ£€æŸ¥å·¥ä½œæµæ˜¯å¦å­˜åœ¨
        if workflow_name not in self.workflows:
            available_workflows = list(self.workflows.keys())
            raise ValueError(f"å·¥ä½œæµ '{workflow_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥ä½œæµ: {available_workflows}")

        workflow_config = self.workflows[workflow_name]

        # æ„å»ºè¡¨æ ¼æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        table_path = os.path.join(self.table_folder, table_filename)
        if not os.path.exists(table_path):
            raise FileNotFoundError(f"è¡¨æ ¼æ–‡ä»¶ {table_path} ä¸å­˜åœ¨")

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
        if table_filename.endswith('.xlsx'):
            df = pd.read_excel(table_path)
        elif table_filename.endswith('.csv'):
            # å°è¯•ä¸åŒçš„ç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312']:
                try:
                    df = pd.read_csv(table_path, encoding=encoding)
                    break
                except UnicodeDecodeError:
                    continue
            else:
                raise ValueError(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ {table_path}ï¼Œè¯·æ£€æŸ¥ç¼–ç æ ¼å¼")
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .xlsx æˆ– .csv æ–‡ä»¶")

        # æ£€æŸ¥æ˜¯å¦æœ‰inputåˆ—
        if 'input' not in df.columns:
            available_columns = list(df.columns)
            raise ValueError(f"è¡¨æ ¼æ–‡ä»¶ä¸­ç¼ºå°‘ 'input' åˆ—ã€‚å¯ç”¨åˆ—: {available_columns}")

        total_records = len(df)
        start_time = time.time()
        completed_count = 0
        results = [None] * total_records  # é¢„åˆ†é…ç»“æœåˆ—è¡¨

        print(f"ğŸ” å¼€å§‹å¤„ç† {total_records} æ¡è®°å½• (å¹¶å‘æ•°: {max_workers})")
        print(f"ğŸ“Š ä½¿ç”¨å·¥ä½œæµ: {workflow_name}")
        print(f"ğŸ“ è¡¨æ ¼æ–‡ä»¶: {table_path}")
        print(f"ğŸ“¤ è¾“å‡ºè·¯å¾„: {self.output_folder}")
        print("-" * 60)

        # åˆå§‹åŒ–è¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback(0, total_records, "å¼€å§‹å¤„ç†æ•°æ®é›†", 0)

        def process_single_record(args):
            """å¤„ç†å•æ¡è®°å½•çš„å‡½æ•°"""
            index, row = args
            input_text = str(row['input'])

            # æ„é€  inputsï¼ˆå»æ‰ input åˆ—ï¼‰
            inputs = {col: row[col] for col in df.columns if col != 'input'}

            try:
                # è°ƒç”¨API
                api_response = self._call_dify_api_with_retry(workflow_config, input_text, inputs)

                # æå–å›ç­”å†…å®¹
                if 'error' not in api_response:
                    answer = api_response.get('answer', '')
                    if not answer and 'data' in api_response:
                        answer = api_response['data'].get('answer', '')
                    if not answer and 'choices' in api_response:
                        answer = api_response['choices'][0].get('message', {}).get('content', '')
                    if not answer:
                        answer = str(api_response)

                    answer = self._normalize_text(answer)
                else:
                    answer = f"APIè°ƒç”¨å¤±è´¥: {api_response['error']}"

                # æ„å»ºç»“æœè®°å½•
                result_row = row.to_dict()
                result_row['model_output'] = answer
                result_row['workflow_name'] = workflow_name
                result_row['timestamp'] = datetime.now().isoformat()

                return index, result_row, None

            except Exception as e:
                error_msg = f"å¤„ç†ç¬¬ {index + 1} æ¡è®°å½•å¤±è´¥: {str(e)}"
                print(f"âŒ {error_msg}")

                # æ„å»ºé”™è¯¯ç»“æœ
                result_row = row.to_dict()
                result_row['model_output'] = f"å¤„ç†å¤±è´¥: {str(e)}"
                result_row['workflow_name'] = workflow_name
                result_row['timestamp'] = datetime.now().isoformat()

                return index, result_row, str(e)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix='dify_api_thread') as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_record, (index, row)): index
                for index, row in df.iterrows()
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_index):
                try:
                    index, result_row, error = future.result()
                    results[index] = result_row

                    with self.lock:
                        completed_count += 1

                        # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
                        elapsed_time = time.time() - start_time
                        if completed_count > 0:
                            avg_time_per_record = elapsed_time / completed_count
                            remaining_records = total_records - completed_count
                            estimated_remaining_time = avg_time_per_record * remaining_records
                        else:
                            estimated_remaining_time = 0

                        print(f"âœ… å®Œæˆç¬¬ {completed_count}/{total_records} æ¡")

                        # æ›´æ–°è¿›åº¦å›è°ƒ
                        if progress_callback:
                            progress_callback(
                                completed_count,
                                total_records,
                                f"å·²å®Œæˆç¬¬ {completed_count}/{total_records} æ¡è®°å½•",
                                estimated_remaining_time
                            )

                except Exception as e:
                    print(f"âŒ å¤„ç†ä»»åŠ¡ç»“æœæ—¶å‡ºé”™: {e}")

        # ä¿å­˜ç»“æœ
        final_results = [r for r in results if r is not None]
        result_df = pd.DataFrame(final_results)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        safe_workflow_name = "".join(c for c in workflow_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_table_name = table_filename.split('.')[0]

        output_filename = f"{safe_workflow_name}_{safe_table_name}_{timestamp}.xlsx"
        output_path = os.path.join(self.output_folder, output_filename)

        result_df.to_excel(output_path, index=False, engine='openpyxl')

        # æœ€ç»ˆå®Œæˆå›è°ƒ
        if progress_callback:
            progress_callback(total_records, total_records, "æ•°æ®å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶", 0)

        print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_path}")
        return output_path
