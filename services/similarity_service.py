import time
import traceback
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

from vector_db.factory import VectorDBFactory
from vector_db.embedding_client import QwenEmbeddingAPI
from config.settings import settings


class SimilarityCalculator:
    """ç›¸ä¼¼åº¦è®¡ç®—å¼•æ“"""

    def __init__(self, db_type: str = None):
        """åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å¼•æ“"""

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
        self.db_client = VectorDBFactory.create_client(db_type)

        # åˆå§‹åŒ–embedding API
        self.embedding_api = QwenEmbeddingAPI()

        print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®åº“ç±»å‹: {settings.vector_db_type}")

    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿æ¥çŠ¶æ€"""
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            collections = self.db_client.list_collections()
            db_status = "connected"
            collections_count = len(collections)
        except Exception as e:
            db_status = f"failed: {str(e)}"
            collections_count = 0

        # æµ‹è¯•Embedding APIè¿æ¥
        try:
            test_embedding = self.embedding_api.encode_texts(["test"])
            embedding_status = "connected" if test_embedding else "failed"
        except Exception as e:
            embedding_status = f"failed: {str(e)}"

        return {
            "vector_db": {
                "type": settings.vector_db_type,
                "status": db_status,
                "collections_count": collections_count
            },
            "embedding_api": {
                "status": embedding_status,
                "base_url": settings.embedding_api_url,
                "model": settings.embedding_model
            }
        }

    def get_collections(self) -> List[str]:
        """è·å–æ‰€æœ‰collectionåˆ—è¡¨"""
        return self.db_client.list_collections()

    def get_collection_fields(self, collection_name: str) -> List[str]:
        """è·å–collectionçš„å­—æ®µåˆ—è¡¨"""
        return self.db_client.get_collection_fields(collection_name)

    def get_collection_data(self, collection_name: str, limit: int = 1000) -> List[Dict]:
        """è·å–collectionçš„æ•°æ®"""
        return self.db_client.get_all_data(collection_name, limit)

    def calculate_cosine_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        try:
            v1 = np.array(vector1)
            v2 = np.array(vector2)

            # æ£€æŸ¥å‘é‡ç»´åº¦æ˜¯å¦åŒ¹é…
            if v1.shape != v2.shape:
                raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: {v1.shape} vs {v2.shape}")

            dot_product = np.dot(v1, v2)
            norm_v1 = np.linalg.norm(v1)
            norm_v2 = np.linalg.norm(v2)

            if norm_v1 == 0 or norm_v2 == 0:
                return 0.0

            similarity = dot_product / (norm_v1 * norm_v2)

            # å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦è¯¯å·®ï¼Œç¡®ä¿ç»“æœåœ¨[-1, 1]èŒƒå›´å†…
            similarity = max(-0.0, min(1.0, similarity))

            return float(similarity)

        except ValueError:
            # é‡æ–°æŠ›å‡ºç»´åº¦ä¸åŒ¹é…ç­‰å€¼é”™è¯¯
            raise
        except Exception as e:
            print(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            raise ValueError(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")

    def calculate_similarity_matrix(self, x_collection: str, y_collection: str,
                                    x_max_items: int = 100, y_max_items: int = 100) -> Dict[str, Any]:
        """è®¡ç®—ä¸¤ä¸ªcollectionä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
        try:
            start_time = time.time()

            print(f"ğŸš€ å¼€å§‹è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ")
            print(f"   X Collection: {x_collection} (æœ€å¤§é¡¹ç›®æ•°: {x_max_items})")
            print(f"   Y Collection: {y_collection} (æœ€å¤§é¡¹ç›®æ•°: {y_max_items})")

            # è·å–æ•°æ®
            print("ğŸ“– è·å–æ•°æ®...")
            x_data = self.get_collection_data(x_collection, x_max_items)
            y_data = self.get_collection_data(y_collection, y_max_items)

            if not x_data or not y_data:
                raise ValueError("æ•°æ®ä¸ºç©º")

            print(f"   Xæ•°æ®æ¡æ•°: {len(x_data)}")
            print(f"   Yæ•°æ®æ¡æ•°: {len(y_data)}")

            # æå–å‘é‡
            x_vectors = []
            y_vectors = []

            for item in x_data:
                if 'dense_vector' in item:
                    x_vectors.append(item['dense_vector'])
                elif 'embedding' in item:
                    x_vectors.append(item['embedding'])
                else:
                    raise ValueError("Xæ•°æ®ä¸­ç¼ºå°‘å‘é‡å­—æ®µ")

            for item in y_data:
                if 'dense_vector' in item:
                    y_vectors.append(item['dense_vector'])
                elif 'embedding' in item:
                    y_vectors.append(item['embedding'])
                else:
                    raise ValueError("Yæ•°æ®ä¸­ç¼ºå°‘å‘é‡å­—æ®µ")

            # æ£€æŸ¥å‘é‡ç»´åº¦æ˜¯å¦ä¸€è‡´
            if x_vectors and y_vectors:
                x_dim = len(x_vectors[0])
                y_dim = len(y_vectors[0])
                if x_dim != y_dim:
                    raise ValueError(
                        f"å‘é‡ç»´åº¦ä¸åŒ¹é…: {x_collection} çš„å‘é‡ç»´åº¦ä¸º {x_dim}, "
                        f"{y_collection} çš„å‘é‡ç»´åº¦ä¸º {y_dim}ã€‚"
                        f"æ— æ³•è®¡ç®—ä¸åŒç»´åº¦å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚"
                    )

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            print("ğŸ”„ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
            similarity_matrix = []
            total_comparisons = len(y_data) * len(x_data)
            completed = 0

            for i, y_vector in enumerate(y_vectors):
                row = []
                for j, x_vector in enumerate(x_vectors):
                    similarity = self.calculate_cosine_similarity(y_vector, x_vector)
                    row.append(similarity)
                    completed += 1

                    if completed % 1000 == 0:
                        progress = (completed / total_comparisons) * 100
                        print(f"   è¿›åº¦: {progress:.1f}% ({completed}/{total_comparisons})")

                similarity_matrix.append(row)

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            flat_similarities = [sim for row in similarity_matrix for sim in row]
            stats = {
                'total_pairs': len(flat_similarities),
                'avg_similarity': float(np.mean(flat_similarities)),
                'min_similarity': float(np.min(flat_similarities)),
                'max_similarity': float(np.max(flat_similarities)),
                'std_similarity': float(np.std(flat_similarities)),
                'high_similarity_count': sum(1 for sim in flat_similarities if sim > 0.8),
                'medium_similarity_count': sum(1 for sim in flat_similarities if 0.5 <= sim <= 0.8),
                'low_similarity_count': sum(1 for sim in flat_similarities if sim < 0.5),
                'compute_time': (time.time() - start_time) * 1000
            }

            end_time = time.time()
            print(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

            # å¤„ç†æ•°æ®
            x_data_processed = []
            for i, item in enumerate(x_data):
                item_copy = {k: v for k, v in item.items() if k not in ['dense_vector', 'embedding']}
                item_copy['order_id'] = i
                x_data_processed.append(item_copy)

            y_data_processed = []
            for i, item in enumerate(y_data):
                item_copy = {k: v for k, v in item.items() if k not in ['dense_vector', 'embedding']}
                item_copy['order_id'] = i
                y_data_processed.append(item_copy)

            # è·å–å¯ç”¨å­—æ®µåˆ—è¡¨
            x_available_fields = [k for k in x_data_processed[0].keys() if k not in ['id']]
            y_available_fields = [k for k in y_data_processed[0].keys() if k not in ['id']]

            return {
                'matrix': similarity_matrix,
                'x_data': x_data_processed,
                'y_data': y_data_processed,
                'x_available_fields': x_available_fields,
                'y_available_fields': y_available_fields,
                'stats': stats,
                'metadata': {
                    'x_collection': x_collection,
                    'y_collection': y_collection,
                    'x_max_items': x_max_items,
                    'y_max_items': y_max_items,
                    'calculation_time': end_time - start_time
                }
            }

        except Exception as e:
            print(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µå¤±è´¥: {e}")
            traceback.print_exc()
            raise

    def find_similar_items(self, collection_name: str, query_text: str,
                           field_name: str, top_k: int = 10) -> List[Dict]:
        """æŸ¥æ‰¾ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„é¡¹ç›®"""
        try:
            # å°†æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–
            query_vectors = self.embedding_api.encode_texts([query_text])
            if not query_vectors:
                raise ValueError("æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–å¤±è´¥")

            query_vector = query_vectors[0]

            # åœ¨æ•°æ®åº“ä¸­æœç´¢
            search_results = self.db_client.query_by_vector(
                collection_name=collection_name,
                query_vector=query_vector,
                top_k=top_k
            )

            # æ ¼å¼åŒ–ç»“æœ
            results = []
            for result in search_results:
                formatted_result = {
                    "id": result.get("id", ""),
                    "text": result.get(field_name, result.get("document", "")),
                    "similarity": result.get("similarity", 0.0),
                    "distance": result.get("distance", 0.0),
                    "metadata": result.get("metadata", {})
                }
                results.append(formatted_result)

            return results

        except Exception as e:
            print(f"âŒ ç›¸ä¼¼é¡¹ç›®æœç´¢å¤±è´¥: {e}")
            raise

    def get_collection_stats(self, collection_name: str) -> Dict[str, Any]:
        """è·å–collectionç»Ÿè®¡ä¿¡æ¯"""
        return self.db_client.get_collection_stats(collection_name)

    def validate_request_data(self, data: Dict[str, Any], required_fields: List[str]) -> Optional[str]:
        """éªŒè¯è¯·æ±‚æ•°æ®"""
        for field in required_fields:
            if field not in data:
                return f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
            if not data[field]:
                return f"å­—æ®µ {field} ä¸èƒ½ä¸ºç©º"
        return None
